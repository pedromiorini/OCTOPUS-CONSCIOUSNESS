# src/tentaculos/tentaculo_grokiana.py

import logging
import json
import asyncio
import hashlib
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime

from .base_tentaculo import BaseTentaculo
from src.cognitive.cerebro import Cerebro
from src.shared.comunicacao import BarramentoEventos

logger = logging.getLogger(__name__)


class FormatoDataset(Enum):
    """Formatos suportados para exporta√ß√£o de datasets."""
    ALPACA = "alpaca"
    SHAREGPT = "sharegpt"
    OPENAI_CHAT = "openai_chat"
    INSTRUCTION_RESPONSE = "instruction_response"


@dataclass
class ParQA:
    """Representa um par de instru√ß√£o/resposta com metadados."""
    instruction: str
    output: str
    input: str = ""
    source_url: Optional[str] = None
    confidence_score: float = 1.0
    timestamp: str = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow().isoformat()
        if self.metadata is None:
            self.metadata = {}

    def to_dict(self, formato: FormatoDataset = FormatoDataset.ALPACA) -> Dict[str, Any]:
        """Converte o par para o formato especificado."""
        if formato == FormatoDataset.ALPACA:
            return {
                "instruction": self.instruction,
                "input": self.input,
                "output": self.output
            }
        elif formato == FormatoDataset.SHAREGPT:
            return {
                "conversations": [
                    {"from": "human", "value": self.instruction},
                    {"from": "gpt", "value": self.output}
                ]
            }
        elif formato == FormatoDataset.OPENAI_CHAT:
            return {
                "messages": [
                    {"role": "user", "content": self.instruction},
                    {"role": "assistant", "content": self.output}
                ]
            }
        return asdict(self)


class WebScraperCognitivo:
    """Extrator inteligente de conte√∫do web usando an√°lise sem√¢ntica."""
    
    def __init__(self, cerebro: Cerebro):
        self.cerebro = cerebro
        self.cache = {}
    
    async def extrair_conteudo(self, topico: str, url: Optional[str] = None) -> Tuple[str, Dict[str, Any]]:
        """
        Extrai conte√∫do relevante sobre um t√≥pico.
        
        Returns:
            Tuple[str, Dict]: (conte√∫do extra√≠do, metadados)
        """
        cache_key = hashlib.md5(topico.encode()).hexdigest()
        
        if cache_key in self.cache:
            logger.info(f"  üì¶ Conte√∫do de '{topico}' recuperado do cache.")
            return self.cache[cache_key]
        
        logger.info(f"  üîç Extraindo conte√∫do sobre '{topico}'...")
        
        # Prompt aprimorado para extra√ß√£o estruturada
        prompt = f"""Voc√™ √© um extrator especializado de conhecimento t√©cnico.
        
Tarefa: Gere um artigo t√©cnico detalhado e estruturado sobre: "{topico}"

Requisitos:
1. Use se√ß√µes claras com subt√≠tulos
2. Inclua defini√ß√µes precisas
3. Forne√ßa exemplos pr√°ticos
4. Cite conceitos relacionados
5. Mantenha rigor t√©cnico
6. Comprimento: 800-1200 palavras

Estrutura sugerida:
## Vis√£o Geral
## Conceitos Fundamentais
## Aplica√ß√µes Pr√°ticas
## T√©cnicas Avan√ßadas
## Considera√ß√µes e Limita√ß√µes

Artigo:"""

        conteudo = self.cerebro.gerar_pensamento(prompt, max_tokens=2000)
        
        metadados = {
            "topico": topico,
            "url_fonte": url,
            "timestamp_extracao": datetime.utcnow().isoformat(),
            "tamanho_caracteres": len(conteudo),
            "tamanho_palavras": len(conteudo.split())
        }
        
        resultado = (conteudo, metadados)
        self.cache[cache_key] = resultado
        
        return resultado


class GeradorDeParesQA:
    """Transformador inteligente de texto em pares instru√ß√£o/resposta."""
    
    def __init__(self, cerebro: Cerebro):
        self.cerebro = cerebro
        self.min_chunk_size = 150
        self.max_chunk_size = 800
        self.min_confidence = 0.6
    
    async def gerar_pares(self, texto: str, metadados: Dict[str, Any]) -> List[ParQA]:
        """
        Gera pares QA de alta qualidade a partir do texto.
        
        Args:
            texto: Texto fonte
            metadados: Metadados da extra√ß√£o
            
        Returns:
            Lista de ParQA validados
        """
        logger.info("  üß† Gerando pares de instru√ß√£o/resposta...")
        
        chunks = self._segmentar_texto_inteligente(texto)
        logger.info(f"    ‚Üí Texto dividido em {len(chunks)} chunks sem√¢nticos")
        
        pares = []
        tarefas = [self._processar_chunk(chunk, metadados, idx) 
                   for idx, chunk in enumerate(chunks)]
        
        # Processamento paralelo dos chunks
        resultados = await asyncio.gather(*tarefas, return_exceptions=True)
        
        for resultado in resultados:
            if isinstance(resultado, ParQA):
                if resultado.confidence_score >= self.min_confidence:
                    pares.append(resultado)
                else:
                    logger.debug(f"    ‚ö†Ô∏è Par descartado (confian√ßa: {resultado.confidence_score:.2f})")
        
        logger.info(f"    ‚úÖ {len(pares)} pares de alta qualidade gerados")
        return pares
    
    def _segmentar_texto_inteligente(self, texto: str) -> List[str]:
        """Segmenta texto respeitando limites sem√¢nticos."""
        # Primeiro tenta dividir por se√ß√µes (##)
        secoes = texto.split('\n## ')
        chunks = []
        
        for secao in secoes:
            if len(secao) < self.min_chunk_size:
                continue
                
            if len(secao) <= self.max_chunk_size:
                chunks.append(secao.strip())
            else:
                # Divide se√ß√µes grandes por par√°grafos
                paragrafos = secao.split('\n\n')
                chunk_atual = ""
                
                for paragrafo in paragrafos:
                    if len(chunk_atual) + len(paragrafo) <= self.max_chunk_size:
                        chunk_atual += "\n\n" + paragrafo if chunk_atual else paragrafo
                    else:
                        if chunk_atual:
                            chunks.append(chunk_atual.strip())
                        chunk_atual = paragrafo
                
                if chunk_atual:
                    chunks.append(chunk_atual.strip())
        
        return [c for c in chunks if len(c) >= self.min_chunk_size]
    
    async def _processar_chunk(self, chunk: str, metadados: Dict, idx: int) -> ParQA:
        """Processa um chunk individual gerando um par QA."""
        prompt = f"""Voc√™ √© um especialista em criar dados de treinamento para modelos de linguagem.

**Tarefa:** Gerar uma instru√ß√£o/pergunta e sua resposta baseada no texto abaixo.

**Texto:**
{chunk}

**Requisitos:**
1. A instru√ß√£o deve ser natural, espec√≠fica e desafiadora
2. A resposta deve reformular o conhecimento do texto (n√£o copiar literalmente)
3. Mantenha precis√£o t√©cnica
4. Use linguagem clara e profissional

**Formato de sa√≠da (JSON v√°lido):**
{{
  "instruction": "sua pergunta ou instru√ß√£o aqui",
  "output": "resposta detalhada aqui",
  "confidence": 0.95,
  "reasoning": "breve explica√ß√£o da qualidade do par"
}}

JSON:"""

        try:
            resposta = self.cerebro.gerar_pensamento(prompt, max_tokens=800)
            
            # Limpeza robusta do JSON
            resposta = resposta.strip()
            if "```json" in resposta:
                resposta = resposta.split("```json")[1].split("```")[0]
            elif "```" in resposta:
                resposta = resposta.split("```")[1].split("```")[0]
            
            dados = json.loads(resposta.strip())
            
            return ParQA(
                instruction=dados["instruction"],
                output=dados["output"],
                source_url=metadados.get("url_fonte"),
                confidence_score=dados.get("confidence", 0.8),
                metadata={
                    "chunk_index": idx,
                    "reasoning": dados.get("reasoning", ""),
                    "topico_fonte": metadados.get("topico")
                }
            )
            
        except Exception as e:
            logger.warning(f"    ‚ö†Ô∏è Erro ao processar chunk {idx}: {e}")
            # Retorna par com confian√ßa baixa para ser filtrado
            return ParQA(
                instruction="erro",
                output="erro",
                confidence_score=0.0
            )


class MontadorDeDataset:
    """Compilador e validador de datasets de treinamento."""
    
    def __init__(self, diretorio_saida: str = "datasets"):
        self.diretorio = Path(diretorio_saida)
        self.diretorio.mkdir(parents=True, exist_ok=True)
    
    def montar_dataset(
        self,
        pares: List[ParQA],
        nome_topico: str,
        formato: FormatoDataset = FormatoDataset.ALPACA
    ) -> Dict[str, Any]:
        """
        Compila pares em um dataset formatado.
        
        Returns:
            Dict com estat√≠sticas e caminho do arquivo
        """
        logger.info(f"  üìä Montando dataset no formato {formato.value}...")
        
        # Valida√ß√£o e filtragem final
        pares_validos = self._validar_pares(pares)
        
        # Gera nome de arquivo √∫nico
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        nome_arquivo = f"{nome_topico}_{formato.value}_{timestamp}.jsonl"
        caminho_completo = self.diretorio / nome_arquivo
        
        # Escreve dataset
        with open(caminho_completo, 'w', encoding='utf-8') as f:
            for par in pares_validos:
                linha = json.dumps(par.to_dict(formato), ensure_ascii=False)
                f.write(linha + '\n')
        
        # Gera arquivo de metadados
        self._salvar_metadados(pares_validos, nome_topico, caminho_completo)
        
        estatisticas = self._gerar_estatisticas(pares_validos)
        
        logger.info(f"    ‚úÖ Dataset salvo: {caminho_completo}")
        logger.info(f"    üìà Estat√≠sticas: {estatisticas['total_exemplos']} exemplos, "
                   f"confian√ßa m√©dia: {estatisticas['confidence_media']:.2f}")
        
        return {
            "caminho": str(caminho_completo),
            "formato": formato.value,
            **estatisticas
        }
    
    def _validar_pares(self, pares: List[ParQA]) -> List[ParQA]:
        """Aplica filtros de qualidade nos pares."""
        validos = []
        
        for par in pares:
            # Filtros de qualidade
            if len(par.instruction) < 20:
                logger.debug(f"    ‚ö†Ô∏è Instru√ß√£o muito curta descartada")
                continue
            
            if len(par.output) < 50:
                logger.debug(f"    ‚ö†Ô∏è Resposta muito curta descartada")
                continue
            
            if par.instruction.lower() == par.output.lower():
                logger.debug(f"    ‚ö†Ô∏è Instru√ß√£o id√™ntica √† resposta descartada")
                continue
            
            validos.append(par)
        
        logger.info(f"    ‚úì {len(validos)}/{len(pares)} pares passaram na valida√ß√£o")
        return validos
    
    def _salvar_metadados(self, pares: List[ParQA], topico: str, caminho_dataset: Path):
        """Salva metadados do dataset para rastreabilidade."""
        metadados = {
            "topico": topico,
            "timestamp_criacao": datetime.utcnow().isoformat(),
            "total_exemplos": len(pares),
            "confidence_scores": [p.confidence_score for p in pares],
            "fonte_dados": "TentaculoGrokiana",
            "versao_pipeline": "2.0"
        }
        
        caminho_meta = caminho_dataset.with_suffix('.meta.json')
        with open(caminho_meta, 'w', encoding='utf-8') as f:
            json.dump(metadados, f, indent=2, ensure_ascii=False)
    
    def _gerar_estatisticas(self, pares: List[ParQA]) -> Dict[str, Any]:
        """Gera estat√≠sticas descritivas do dataset."""
        if not pares:
            return {"total_exemplos": 0}
        
        tamanhos_inst = [len(p.instruction) for p in pares]
        tamanhos_out = [len(p.output) for p in pares]
        confidences = [p.confidence_score for p in pares]
        
        return {
            "total_exemplos": len(pares),
            "confidence_media": sum(confidences) / len(confidences),
            "confidence_min": min(confidences),
            "tamanho_medio_instrucao": sum(tamanhos_inst) / len(tamanhos_inst),
            "tamanho_medio_resposta": sum(tamanhos_out) / len(tamanhos_out),
        }


class TentaculoGrokiana(BaseTentaculo):
    """
    Especialista em transformar conhecimento externo em datasets de treinamento
    otimizados para fine-tuning de modelos de linguagem.
    
    Pipeline:
    1. WebScraperCognitivo: Extra√ß√£o inteligente de conte√∫do
    2. GeradorDeParesQA: Transforma√ß√£o em pares instru√ß√£o/resposta
    3. MontadorDeDataset: Compila√ß√£o e valida√ß√£o do dataset final
    """
    
    def __init__(self, cerebro: Cerebro, barramento: BarramentoEventos):
        super().__init__("Grokiana", cerebro, barramento)
        self.scraper = WebScraperCognitivo(cerebro)
        self.gerador = GeradorDeParesQA(cerebro)
        self.montador = MontadorDeDataset()
        logger.info("üìö Tent√°culo Grokiana (Minerador de Conhecimento v2.0) instanciado.")
    
    async def pode_executar(self, tarefa: str) -> bool:
        """Verifica se a tarefa √© de compet√™ncia do Grokiana."""
        palavras_chave = [
            "gere um dataset",
            "criar dataset",
            "treino com grokipedia",
            "minere conhecimento",
            "extrair conhecimento",
            "preparar dados de treinamento"
        ]
        return any(palavra in tarefa.lower() for palavra in palavras_chave)
    
    async def executar_tarefa(self, tarefa: str, **kwargs) -> Dict[str, Any]:
        """
        Executa o pipeline completo de minera√ß√£o e compila√ß√£o de dataset.
        
        Args:
            tarefa: Descri√ß√£o da tarefa
            **kwargs: Par√¢metros opcionais
                - formato: FormatoDataset (default: ALPACA)
                - url_fonte: URL espec√≠fica para extra√ß√£o
                
        Returns:
            Dict com resultado da opera√ß√£o e metadados
        """
        logger.info(f"üöÄ Grokiana: Iniciando miss√£o para '{tarefa}'")
        
        try:
            # Extrai t√≥pico da tarefa
            topico = self._extrair_topico(tarefa)
            formato = kwargs.get('formato', FormatoDataset.ALPACA)
            url = kwargs.get('url_fonte')
            
            logger.info(f"  üìå T√≥pico identificado: '{topico}'")
            logger.info(f"  üìã Formato de sa√≠da: {formato.value}")
            
            # FASE 1: Extra√ß√£o de Conte√∫do
            texto_bruto, metadados = await self.scraper.extrair_conteudo(topico, url)
            
            # FASE 2: Gera√ß√£o de Pares QA
            pares_qa = await self.gerador.gerar_pares(texto_bruto, metadados)
            
            if not pares_qa:
                return {
                    "sucesso": False,
                    "erro": "Nenhum par de qualidade foi gerado",
                    "topico": topico
                }
            
            # FASE 3: Montagem do Dataset
            resultado_dataset = self.montador.montar_dataset(
                pares_qa,
                topico.replace(' ', '_'),
                formato
            )
            
            # Emite evento de conclus√£o
            await self.barramento.emitir("dataset_criado", {
                "tentaculo": self.nome,
                "topico": topico,
                **resultado_dataset
            })
            
            return {
                "sucesso": True,
                "mensagem": f"Dataset de treinamento gerado com sucesso sobre '{topico}'",
                "topico": topico,
                **resultado_dataset
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro no TentaculoGrokiana: {e}", exc_info=True)
            return {
                "sucesso": False,
                "erro": str(e),
                "tipo_erro": type(e).__name__
            }
    
    def _extrair_topico(self, tarefa: str) -> str:
        """Extrai o t√≥pico principal da descri√ß√£o da tarefa."""
        # Remove palavras de comando comuns
        palavras_remover = [
            "gere um dataset sobre",
            "criar dataset sobre",
            "minere conhecimento sobre",
            "extrair conhecimento sobre"
        ]
        
        topico = tarefa.lower()
        for palavra in palavras_remover:
            topico = topico.replace(palavra, "")
        
        return topico.strip()
    
    def get_estatisticas(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas de uso do tent√°culo."""
        # Conta arquivos no diret√≥rio de datasets
        arquivos_dataset = list(self.montador.diretorio.glob("*.jsonl"))
        
        return {
            "total_datasets_gerados": len(arquivos_dataset),
            "diretorio_saida": str(self.montador.diretorio),
            "formatos_suportados": [f.value for f in FormatoDataset],
            "cache_extraidor": len(self.scraper.cache)
        }